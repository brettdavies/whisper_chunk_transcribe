# Standard Libraries

# CLI, Logging, Configuration
from loguru import logger

# Third Party Libraries
import torch
from faster_whisper import WhisperModel

# First Party Libraries
from .logger_config import LoggerConfig
from .metadata import Metadata

class TokenTester:
    def __init__(self, model_dir: str):
        try:

            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"Is CUDA Device Available? {self.device == 'cuda'}")

            # Initialize the WhisperModel
            self.model = WhisperModel(model_dir, device=self.device)

            # Access the tokenizer
            self.tokenizer = self.model.hf_tokenizer
        
        except Exception as e:
            logger.error(f"Error initializing WhisperModel: {e}")

    async def determine_token_length(self):
        # Determine the token length of each word in the prompt_dict
        prompt_dict = Metadata.token_lengths
        for key in prompt_dict:
            prompt_dict[key] = len(self.tokenizer.encode(key))
        logger.info(f"Prompt dictionary:\n{prompt_dict}")

    async def test_prompt_tokens(self, audio_file: str, prompt: str):
        # Example prompt (you can replace this with your own)
        prompt = "Baseball game commentary transcription. Focus on accurate recognition of key terms like pitch, strikeout, home run, etc."

        # Tokenize the prompt
        prompt_tokens = self.tokenizer.encode(prompt)

        # Set up an example of an audio file for transcription (replace 'audio_file_path' with your actual file)
        audio_file = audio_file

        # Transcribe the audio with the initial_prompt
        segments_with_prompt, _ = self.model.transcribe(audio_file, initial_prompt=prompt)

        # Count the tokens generated by the transcription (with prompt)
        transcription_tokens_with_prompt = 0
        for segment in segments_with_prompt:
            transcription_tokens_with_prompt += len(self.tokenizer.encode(segment.text))

        # Total tokens (prompt + transcription with prompt)
        total_tokens_with_prompt = len(prompt_tokens) + transcription_tokens_with_prompt

        logger.info(f"--- Results WITH Initial Prompt ---")
        logger.info(f"Number of tokens in the prompt: {transcription_tokens_with_prompt}")
        logger.info(f"Number of tokens in the prompted transcription: {transcription_tokens_with_prompt}")
        logger.info(f"Total tokens (prompt + prompted transcription): {total_tokens_with_prompt}")

        # Transcribe the audio without the initial_prompt
        segments_without_prompt, _ = self.model.transcribe(audio_file)

        # Count the tokens generated by the transcription (without prompt)
        transcription_tokens_without_prompt = 0
        for segment in segments_without_prompt:
            transcription_tokens_without_prompt += len(self.tokenizer.encode(segment.text))

        # Total tokens (transcription without prompt)
        total_tokens_without_prompt = transcription_tokens_without_prompt

        logger.info(f"--- Results WITHOUT Initial Prompt ---")
        logger.info(f"Number of tokens in the unprompted transcription: {transcription_tokens_without_prompt}")
        logger.info(f"Total tokens (transcription only): {total_tokens_without_prompt}")

        # Check if total with prompt exceeds the 255-token limit
        if total_tokens_with_prompt > 255:
            logger.info(f"Warning: Total tokens (with prompt) exceed the 255-token limit by {total_tokens_with_prompt - 255} tokens.")
        else:
            logger.info("The total token count (with prompt) is within the 255-token limit.")

        # Check if total without prompt exceeds the 255-token limit
        if total_tokens_without_prompt > 255:
            logger.info(f"Warning: Total tokens (without prompt) exceed the 255-token limit by {total_tokens_without_prompt - 255} tokens.")
        else:
            logger.info("The total token count (without prompt) is within the 255-token limit.")

        # Check if the total tokens with prompt are less than the total tokens without prompt
        if total_tokens_with_prompt < total_tokens_without_prompt:
            logger.info(f"Warning: The total token count (with prompt) is less than the total token count (without prompt) by {total_tokens_without_prompt - total_tokens_with_prompt} tokens.")
        else:
            logger.info("The total token count (with prompt) is greater than or equal to the total token count (without prompt).")
